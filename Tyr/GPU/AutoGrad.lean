import Tyr.AutoGrad
import Tyr.GPU.Codegen.IR

namespace Tyr.GPU.AD

open Tyr.AD
open Tyr.GPU.Codegen

/-- 
  Linear Intermediate Representation (LIR).
  Represents the linearized execution trace (JVP logic).
-/
inductive LinearInst where
  | id (dout din : VarId)
  | add (dout din1 din2 : VarId)
  | sub (dout din1 din2 : VarId)
  | mul (dout din1 din2 : VarId) (primal1 primal2 : VarId)
  | div (dout din1 din2 : VarId) (primal1 primal2 : VarId)
  | exp (dout din : VarId) (primalY : VarId)
  | log (dout din : VarId) (primalX : VarId)
  | tanh (dout din : VarId) (primalY : VarId)
  | broadcast (axis : BroadcastAxis) (dout din : VarId)
  | sum (axis : ReduceAxis) (dout din : VarId)
  | mma (trans : MMATranspose) (dout da db dc : VarId) (primalA primalB : VarId)
  | loop (v : VarId) (lo hi : Nat) (body : Array LinearInst)
  | custom (name : String) (dout : VarId) (dins : Array VarId) (ctx : Array VarId)
  deriving Repr, Inhabited

/-- Convert BroadcastAxis to ReduceAxis for VJP -/
def broadcastToReduceAxis : BroadcastAxis → ReduceAxis
  | .Row => .Col
  | .Col => .Row

/-- Trace State -/
structure TraceState where
  primalStmts : Array KStmt := #[]
  linearTrace : Array LinearInst := #[]

abbrev TraceM := StateT TraceState ADM

def emitPrimal (s : KStmt) : TraceM Unit := 
  modify fun st => { st with primalStmts := st.primalStmts.push s }

def emitLinear (l : LinearInst) : TraceM Unit :=
  modify fun st => { st with linearTrace := st.linearTrace.push l }

/-- Lift ADM actions to TraceM -/
def liftADM (m : ADM α) : TraceM α := 
  liftM m

/-- Linearize a statement -/
partial def linearizeStmt (s : KStmt) : TraceM Unit := do
  match s with
  | .declRT v _ _ _ _ =>
    let dv ← liftADM getFreshGpuVar
    liftADM $ setGpuTangent v dv
    emitPrimal s
    
  | .declST v _ _ _ _ =>
    let dv ← liftADM getFreshGpuVar
    liftADM $ setGpuTangent v dv
    emitPrimal s

  | .declRV v _ _ =>
    let dv ← liftADM getFreshGpuVar
    liftADM $ setGpuTangent v dv
    emitPrimal s

  | .declSV v _ _ =>
    let dv ← liftADM getFreshGpuVar
    liftADM $ setGpuTangent v dv
    emitPrimal s

  | .load dst src =>
    let ddst ← liftADM getFreshGpuVar
    let dsrc ← liftADM $ getGpuTangent src
    liftADM $ setGpuTangent dst ddst
    emitPrimal s
    emitLinear $ .id ddst dsrc

  | .store dst src =>
    let ddst ← liftADM $ getGpuTangent dst
    let dsrc ← liftADM $ getGpuTangent src
    emitPrimal s
    emitLinear $ .id ddst dsrc

  | .binary op dst a b =>
    let da ← liftADM $ getGpuTangent a
    let db ← liftADM $ getGpuTangent b
    let ddst ← liftADM getFreshGpuVar
    liftADM $ setGpuTangent dst ddst
    emitPrimal s
    
    -- Check for custom rule first
    let opName := "BinaryOp." ++ toString op
    let rule? ← liftADM $ liftM $ getGpuVJPRule opName
    match rule? with
    | some _ => 
      -- If custom rule exists, emit custom LIR instruction.
      -- We assume custom rules take (dout, [da, db], [a, b]) for binary ops.
      -- This requires convention. For now, just emit custom.
      emitLinear $ .custom opName ddst #[da, db] #[a, b]
    | none =>
      match op with
      | .Add => emitLinear $ .add ddst da db
      | .Sub => emitLinear $ .sub ddst da db
      | .Mul => emitLinear $ .mul ddst da db a b
      | .Div => emitLinear $ .div ddst da db a b
      | _ => pure ()

  | .unary op dst src =>
    let dsrc ← liftADM $ getGpuTangent src
    let ddst ← liftADM getFreshGpuVar
    liftADM $ setGpuTangent dst ddst
    emitPrimal s
    
    let opName := "UnaryOp." ++ toString op
    let rule? ← liftADM $ liftM $ getGpuVJPRule opName
    match rule? with
    | some _ =>
      emitLinear $ .custom opName ddst #[dsrc] #[src, dst] -- Pass input and output as context
    | none =>
      match op with
      | .Copy => emitLinear $ .id ddst dsrc
      | .Exp => emitLinear $ .exp ddst dsrc dst
      | .Log => emitLinear $ .log ddst dsrc src
      | .Tanh => emitLinear $ .tanh ddst dsrc dst
      | _ => pure ()

  | .broadcast axis dst vec =>
    let dvec ← liftADM $ getGpuTangent vec
    let ddst ← liftADM getFreshGpuVar
    liftADM $ setGpuTangent dst ddst
    emitPrimal s
    emitLinear $ .broadcast axis ddst dvec

  | .reduce op axis dst src =>
    let dsrc ← liftADM $ getGpuTangent src
    let ddst ← liftADM getFreshGpuVar
    liftADM $ setGpuTangent dst ddst
    emitPrimal s
    match op with
    | .Sum => emitLinear $ .sum axis ddst dsrc
    | _ => pure ()

  | .mma trans dst a b c =>
    let da ← liftADM $ getGpuTangent a
    let db ← liftADM $ getGpuTangent b
    let dc ← liftADM $ getGpuTangent c
    let ddst ← liftADM getFreshGpuVar
    liftADM $ setGpuTangent dst ddst
    emitPrimal s
    emitLinear $ .mma trans ddst da db dc a b

  | .forLoop v lo hi body =>
    let savedState ← get
    set { savedState with primalStmts := #[], linearTrace := #[] }
    body.forM linearizeStmt
    let innerState ← get
    set { savedState with 
          primalStmts := savedState.primalStmts.push (.forLoop v lo hi innerState.primalStmts)
          linearTrace := savedState.linearTrace.push (.loop v lo hi innerState.linearTrace) }

  | other => emitPrimal other

/-- Transpose a Linear Trace to produce VJP statements -/
partial def transposeTrace (trace : Array LinearInst) : ADM (Array KStmt) := do
  let mut stmts := #[]
  
  for inst in trace.reverse do
    match inst with
    | .id dout din =>
      let ddout ← getGpuCotangent dout
      let ddin ← getGpuCotangent din
      stmts := stmts.push (.binary .Add ddin ddin ddout)

    | .add dout din1 din2 =>
      let ddout ← getGpuCotangent dout
      let ddin1 ← getGpuCotangent din1
      let ddin2 ← getGpuCotangent din2
      stmts := stmts.push (.binary .Add ddin1 ddin1 ddout)
      stmts := stmts.push (.binary .Add ddin2 ddin2 ddout)

    | .sub dout din1 din2 =>
      let ddout ← getGpuCotangent dout
      let ddin1 ← getGpuCotangent din1
      let ddin2 ← getGpuCotangent din2
      stmts := stmts.push (.binary .Add ddin1 ddin1 ddout)
      stmts := stmts.push (.binary .Sub ddin2 ddin2 ddout)

    | .mul dout da db a b =>
      let ddout ← getGpuCotangent dout
      let dda ← getGpuCotangent da
      let ddb ← getGpuCotangent db
      let t1 ← getFreshGpuVar
      let t2 ← getFreshGpuVar
      stmts := stmts.push (.binary .Mul t1 ddout b)
      stmts := stmts.push (.binary .Add dda dda t1)
      stmts := stmts.push (.binary .Mul t2 ddout a)
      stmts := stmts.push (.binary .Add ddb ddb t2)

    | .div dout da db _a b =>
      let ddout ← getGpuCotangent dout
      let dda ← getGpuCotangent da
      let _ddb ← getGpuCotangent db
      let inv_b ← getFreshGpuVar
      let t1 ← getFreshGpuVar
      -- da += dout * (1/b)
      stmts := stmts.push (.unary .Recip inv_b b)
      stmts := stmts.push (.binary .Mul t1 ddout inv_b)
      stmts := stmts.push (.binary .Add dda dda t1)
      -- db calculation omitted for brevity (requires -a/b^2)
      pure ()

    | .exp dout din y =>
      let ddout ← getGpuCotangent dout
      let ddin ← getGpuCotangent din
      let t1 ← getFreshGpuVar
      stmts := stmts.push (.binary .Mul t1 ddout y)
      stmts := stmts.push (.binary .Add ddin ddin t1)

    | .log dout din x =>
      let ddout ← getGpuCotangent dout
      let ddin ← getGpuCotangent din
      let t1 ← getFreshGpuVar
      stmts := stmts.push (.binary .Div t1 ddout x)
      stmts := stmts.push (.binary .Add ddin ddin t1)

    | .tanh dout din y =>
      let ddout ← getGpuCotangent dout
      let ddin ← getGpuCotangent din
      let t1 ← getFreshGpuVar
      let t2 ← getFreshGpuVar
      -- ddin += ddout * (1 - y^2)
      stmts := stmts.push (.binary .Mul t1 y y)
      stmts := stmts.push (.scalarMul t1 t1 (-1.0))
      stmts := stmts.push (.scalarAdd t1 t1 1.0)
      stmts := stmts.push (.binary .Mul t2 ddout t1)
      stmts := stmts.push (.binary .Add ddin ddin t2)

    | .broadcast axis dout din =>
      let ddout ← getGpuCotangent dout
      let ddin ← getGpuCotangent din
      let reduceAxis := broadcastToReduceAxis axis
      stmts := stmts.push (.reduceAccum .Sum reduceAxis ddin ddout ddin)

    | .sum _axis dout din =>
      let _ddout ← getGpuCotangent dout
      let _ddin ← getGpuCotangent din
      -- Requires broadcast accumulator
      pure ()

    | .mma _trans dout _da _db dc _a _b =>
      let ddout ← getGpuCotangent dout
      let ddc ← getGpuCotangent dc
      stmts := stmts.push (.binary .Add ddc ddc ddout)

    | .loop v lo hi body =>
      let transBody ← transposeTrace body
      stmts := stmts.push (.forLoop v lo hi transBody)

    | .custom name dout dins ctx =>
      let rule? ← liftM $ getGpuVJPRule name
      match rule? with
      | some rule =>
        let ddout ← getGpuCotangent dout
        let mut ddins := #[]
        for din in dins do
          ddins := ddins.push (← getGpuCotangent din)
        
        let customStmts ← rule ddout ddins ctx
        stmts := stmts ++ customStmts
      | none => pure ()

  return stmts

end Tyr.GPU.AD
