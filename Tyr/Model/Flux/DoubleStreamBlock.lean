/-
  Tyr/Model/Flux/DoubleStreamBlock.lean

  Double-stream transformer block for Flux.
  Processes image and text tokens with separate paths but joint attention.
-/
import Tyr.Torch
import Tyr.TensorStruct
import Tyr.Module.Core
import Tyr.Module.Derive
import Tyr.Module.LayerNorm
import Tyr.Model.Flux.Config
import Tyr.Model.Flux.RoPE
import Tyr.Model.Flux.Modulation
import Tyr.Model.Flux.QKNorm

namespace torch.flux

/-- Self-attention layer for double-stream block -/
structure SelfAttention (hidden_size num_heads head_dim : UInt64) where
  /-- QKV projection: [hidden * 3, hidden] -/
  qkv : T #[num_heads * head_dim * 3, hidden_size]
  /-- QK normalization -/
  norm : QKNorm head_dim
  /-- Output projection: [hidden, hidden] -/
  proj : T #[hidden_size, num_heads * head_dim]
  deriving TensorStruct

/-- SwiGLU MLP for double-stream block -/
structure SwiGLUMLP (hidden_size mlp_hidden : UInt64) where
  /-- Linear in: gate and up projection fused -/
  w1 : T #[mlp_hidden * 2, hidden_size]
  /-- Linear out: down projection -/
  w2 : T #[hidden_size, mlp_hidden]
  deriving TensorStruct

/-- Double-stream transformer block.
    Processes image and text tokens with separate paths but joint attention. -/
structure DoubleStreamBlock (hidden_size num_heads head_dim mlp_hidden : UInt64) where
  /-- Image path normalization -/
  img_norm1 : LayerNorm hidden_size
  /-- Image self-attention -/
  img_attn : SelfAttention hidden_size num_heads head_dim
  /-- Image MLP normalization -/
  img_norm2 : LayerNorm hidden_size
  /-- Image MLP -/
  img_mlp : SwiGLUMLP hidden_size mlp_hidden

  /-- Text path normalization -/
  txt_norm1 : LayerNorm hidden_size
  /-- Text self-attention -/
  txt_attn : SelfAttention hidden_size num_heads head_dim
  /-- Text MLP normalization -/
  txt_norm2 : LayerNorm hidden_size
  /-- Text MLP -/
  txt_mlp : SwiGLUMLP hidden_size mlp_hidden

  deriving TensorStruct

namespace DoubleStreamBlock

/-- Initialize self-attention -/
def initSelfAttention (hidden_size num_heads head_dim : UInt64)
    : IO (SelfAttention hidden_size num_heads head_dim) := do
  let std := Float.sqrt (2.0 / hidden_size.toFloat)
  let qkv ← torch.randn #[num_heads * head_dim * 3, hidden_size]
  let proj ← torch.randn #[hidden_size, num_heads * head_dim]
  let norm := QKNorm.init head_dim
  pure {
    qkv := autograd.set_requires_grad (mul_scalar qkv std) true
    norm
    proj := autograd.set_requires_grad (mul_scalar proj std) true
  }

/-- Initialize SwiGLU MLP -/
def initMLP (hidden_size mlp_hidden : UInt64)
    : IO (SwiGLUMLP hidden_size mlp_hidden) := do
  let std := Float.sqrt (2.0 / hidden_size.toFloat)
  let w1 ← torch.randn #[mlp_hidden * 2, hidden_size]
  let w2 ← torch.randn #[hidden_size, mlp_hidden]
  pure {
    w1 := autograd.set_requires_grad (mul_scalar w1 std) true
    w2 := autograd.set_requires_grad (mul_scalar w2 std) true
  }

/-- Initialize double-stream block -/
def init (hidden_size num_heads head_dim mlp_hidden : UInt64)
    : IO (DoubleStreamBlock hidden_size num_heads head_dim mlp_hidden) := do
  let img_norm1 := LayerNorm.init hidden_size
  let img_attn ← initSelfAttention hidden_size num_heads head_dim
  let img_norm2 := LayerNorm.init hidden_size
  let img_mlp ← initMLP hidden_size mlp_hidden

  let txt_norm1 := LayerNorm.init hidden_size
  let txt_attn ← initSelfAttention hidden_size num_heads head_dim
  let txt_norm2 := LayerNorm.init hidden_size
  let txt_mlp ← initMLP hidden_size mlp_hidden

  pure {
    img_norm1, img_attn, img_norm2, img_mlp
    txt_norm1, txt_attn, txt_norm2, txt_mlp
  }

/-- Forward pass for double-stream block.
    img: [batch, img_seq, hidden_size]
    txt: [batch, txt_seq, hidden_size]
    vec: [batch, hidden_size] - timestep embedding
    pe: RoPE embeddings
    Returns: (img_out, txt_out) -/
def forward {batch img_seq txt_seq hidden_size num_heads head_dim mlp_hidden : UInt64}
    (block : DoubleStreamBlock hidden_size num_heads head_dim mlp_hidden)
    (img : T #[batch, img_seq, hidden_size])
    (txt : T #[batch, txt_seq, hidden_size])
    (img_pe txt_pe : T #[])  -- RoPE for image and text
    (mod_img : T #[batch, 6, hidden_size])
    (mod_txt : T #[batch, 6, hidden_size])
    : T #[batch, img_seq, hidden_size] × T #[batch, txt_seq, hidden_size] :=
  -- Split modulation into two triplets for attention and MLP
  let img_mod1 := data.slice mod_img 1 0 3
  let img_mod2 := data.slice mod_img 1 3 3
  let txt_mod1 := data.slice mod_txt 1 0 3
  let txt_mod2 := data.slice mod_txt 1 3 3

  -- Modulation order is (shift, scale, gate)
  let img_mod1_shift := reshape (data.slice img_mod1 1 0 1) #[batch, hidden_size]
  let img_mod1_scale := reshape (data.slice img_mod1 1 1 1) #[batch, hidden_size]
  let img_mod1_gate := reshape (data.slice img_mod1 1 2 1) #[batch, hidden_size]
  let img_mod2_shift := reshape (data.slice img_mod2 1 0 1) #[batch, hidden_size]
  let img_mod2_scale := reshape (data.slice img_mod2 1 1 1) #[batch, hidden_size]
  let img_mod2_gate := reshape (data.slice img_mod2 1 2 1) #[batch, hidden_size]

  let txt_mod1_shift := reshape (data.slice txt_mod1 1 0 1) #[batch, hidden_size]
  let txt_mod1_scale := reshape (data.slice txt_mod1 1 1 1) #[batch, hidden_size]
  let txt_mod1_gate := reshape (data.slice txt_mod1 1 2 1) #[batch, hidden_size]
  let txt_mod2_shift := reshape (data.slice txt_mod2 1 0 1) #[batch, hidden_size]
  let txt_mod2_scale := reshape (data.slice txt_mod2 1 1 1) #[batch, hidden_size]
  let txt_mod2_gate := reshape (data.slice txt_mod2 1 2 1) #[batch, hidden_size]

  -- Image path: norm1 + modulation
  let img_norm := block.img_norm1.forward3d img
  let img_mod := applyModulation img_norm img_mod1_scale img_mod1_shift

  -- Text path: norm1 + modulation
  let txt_norm := block.txt_norm1.forward3d txt
  let txt_mod := applyModulation txt_norm txt_mod1_scale txt_mod1_shift

  -- Project to Q, K, V for both
  let img_qkv := linear3d img_mod block.img_attn.qkv
  let txt_qkv := linear3d txt_mod block.txt_attn.qkv

  -- Reshape QKV
  let img_qkv := reshape img_qkv #[batch, img_seq, 3, num_heads, head_dim]
  let txt_qkv := reshape txt_qkv #[batch, txt_seq, 3, num_heads, head_dim]

  let img_q := reshape (data.slice img_qkv 2 0 1) #[batch, img_seq, num_heads, head_dim]
  let img_k := reshape (data.slice img_qkv 2 1 1) #[batch, img_seq, num_heads, head_dim]
  let img_v := reshape (data.slice img_qkv 2 2 1) #[batch, img_seq, num_heads, head_dim]

  let txt_q := reshape (data.slice txt_qkv 2 0 1) #[batch, txt_seq, num_heads, head_dim]
  let txt_k := reshape (data.slice txt_qkv 2 1 1) #[batch, txt_seq, num_heads, head_dim]
  let txt_v := reshape (data.slice txt_qkv 2 2 1) #[batch, txt_seq, num_heads, head_dim]

  -- QK normalization
  let (img_q, img_k) := block.img_attn.norm.forward img_q img_k
  let (txt_q, txt_k) := block.txt_attn.norm.forward txt_q txt_k

  -- Apply RoPE
  let img_q := flux.applyRope img_q img_pe
  let img_k := flux.applyRope img_k img_pe
  let txt_q := flux.applyRope txt_q txt_pe
  let txt_k := flux.applyRope txt_k txt_pe

  -- Concatenate for joint attention
  -- q: [batch, img_seq + txt_seq, num_heads, head_dim]
  let q := nn.cat txt_q img_q 1
  let k := nn.cat txt_k img_k 1
  let v := nn.cat txt_v img_v 1

  -- Transpose for attention
  let q := nn.transpose_for_attention q
  let k := nn.transpose_for_attention k
  let v := nn.transpose_for_attention v

  -- Joint attention
  let attn := nn.scaled_dot_product_attention q k v 0.0 false

  -- Transpose back
  let attn := nn.transpose_from_attention attn
  -- Split back to img and txt
  let txt_attn := data.slice attn 1 0 txt_seq
  let img_attn := data.slice attn 1 txt_seq img_seq
  let img_attn := reshape img_attn #[batch, img_seq, num_heads * head_dim]
  let txt_attn := reshape txt_attn #[batch, txt_seq, num_heads * head_dim]

  -- Output projections
  let img_attn := linear3d img_attn block.img_attn.proj
  let txt_attn := linear3d txt_attn block.txt_attn.proj

  -- Apply gates
  let img_mod1_gate := nn.unsqueeze img_mod1_gate 1
  let img_mod1_gate := nn.expand img_mod1_gate #[batch, img_seq, hidden_size]
  let txt_mod1_gate := nn.unsqueeze txt_mod1_gate 1
  let txt_mod1_gate := nn.expand txt_mod1_gate #[batch, txt_seq, hidden_size]

  -- Residual + gated attention
  let img := img + img_mod1_gate * img_attn
  let txt := txt + txt_mod1_gate * txt_attn

  -- MLP paths with second modulation
  let img_mlp_in := block.img_norm2.forward3d img
  let img_mlp_in := applyModulation img_mlp_in img_mod2_scale img_mod2_shift
  let img_mlp_proj := linear3d img_mlp_in block.img_mlp.w1
  let img_mlp_proj := reshape img_mlp_proj #[batch, img_seq, 2, mlp_hidden]
  let img_gate := reshape (data.slice img_mlp_proj 2 0 1) #[batch, img_seq, mlp_hidden]
  let img_up := reshape (data.slice img_mlp_proj 2 1 1) #[batch, img_seq, mlp_hidden]
  let img_mlp_out := nn.silu img_gate * img_up
  let img_mlp_out := linear3d img_mlp_out block.img_mlp.w2
  let img_mod2_gate := nn.unsqueeze img_mod2_gate 1
  let img_mod2_gate := nn.expand img_mod2_gate #[batch, img_seq, hidden_size]
  let img := img + img_mod2_gate * img_mlp_out

  let txt_mlp_in := block.txt_norm2.forward3d txt
  let txt_mlp_in := applyModulation txt_mlp_in txt_mod2_scale txt_mod2_shift
  let txt_mlp_proj := linear3d txt_mlp_in block.txt_mlp.w1
  let txt_mlp_proj := reshape txt_mlp_proj #[batch, txt_seq, 2, mlp_hidden]
  let txt_gate := reshape (data.slice txt_mlp_proj 2 0 1) #[batch, txt_seq, mlp_hidden]
  let txt_up := reshape (data.slice txt_mlp_proj 2 1 1) #[batch, txt_seq, mlp_hidden]
  let txt_mlp_out := nn.silu txt_gate * txt_up
  let txt_mlp_out := linear3d txt_mlp_out block.txt_mlp.w2
  let txt_mod2_gate := nn.unsqueeze txt_mod2_gate 1
  let txt_mod2_gate := nn.expand txt_mod2_gate #[batch, txt_seq, hidden_size]
  let txt := txt + txt_mod2_gate * txt_mlp_out

  (img, txt)

end DoubleStreamBlock

end torch.flux
