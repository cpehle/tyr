/-
  Tyr/Model/Flux/Modulation.lean

  Adaptive modulation for Flux diffusion model.
  Computes scale/shift/gate parameters from timestep embedding.
-/
import Tyr.Torch
import Tyr.TensorStruct
import Tyr.Module.Core
import Tyr.Module.Derive

namespace torch.flux

/-- Modulation output: shift, scale, gate triplets.
    For double blocks: 6 values (2 triplets for img and txt)
    For single blocks: 3 values (1 triplet) -/
structure ModulationOutput (batch num_mods hidden_size : UInt64) where
  values : T #[batch, num_mods, hidden_size]

/-- Modulation layer: projects timestep embedding to shift/scale/gate.
    For double blocks: outputs 6 modulation vectors
    For single blocks: outputs 3 modulation vectors -/
structure Modulation (hidden_size : UInt64) (isDouble : Bool) where
  /-- Linear projection from hidden_size to num_mods * hidden_size -/
  lin : T #[if isDouble then 6 * hidden_size else 3 * hidden_size, hidden_size]
  deriving TensorStruct

namespace Modulation

/-- Number of modulation outputs -/
def numMods (isDouble : Bool) : UInt64 :=
  if isDouble then 6 else 3

/-- Initialize modulation layer -/
def init (hidden_size : UInt64) (isDouble : Bool := false) : IO (Modulation hidden_size isDouble) := do
  let outDim := if isDouble then 6 * hidden_size else 3 * hidden_size
  let std := Float.sqrt (2.0 / hidden_size.toFloat)
  let w ‚Üê torch.randn #[outDim, hidden_size]
  pure { lin := autograd.set_requires_grad (mul_scalar w std) true }

/-- Initialize modulation with zero weights (for dummy/placeholder modulation).
    Used when modulation is at model level instead of per-block. -/
def initZero (hidden_size : UInt64) (isDouble : Bool := false) : Modulation hidden_size isDouble :=
  let outDim := if isDouble then 6 * hidden_size else 3 * hidden_size
  { lin := autograd.set_requires_grad (torch.zeros #[outDim, hidden_size]) false }

/-- Forward pass: compute modulation from timestep embedding.
    Input vec: [batch, hidden_size] (timestep embedding)
    Output: [batch, num_mods, hidden_size] -/
def forward {batch hidden_size : UInt64} (isDouble : Bool)
    (mod : Modulation hidden_size isDouble)
    (vec : T #[batch, hidden_size])
    : T #[batch, if isDouble then 6 else 3, hidden_size] :=
  -- Apply SiLU activation first
  let vec := nn.silu vec
  -- Project to modulation vectors
  let out := linear vec mod.lin  -- [batch, num_mods * hidden_size]
  -- Reshape to [batch, num_mods, hidden_size]
  let numMods := if isDouble then 6 else 3
  reshape out #[batch, numMods, hidden_size]

end Modulation

/-- Apply modulation to a tensor.
    x: [batch, seq, hidden_size]
    scale, shift: [batch, hidden_size]
    Returns: scale * x + shift -/
def applyModulation {batch seq hidden_size : UInt64}
    (x : T #[batch, seq, hidden_size])
    (scale : T #[batch, hidden_size])
    (shift : T #[batch, hidden_size])
    : T #[batch, seq, hidden_size] :=
  -- Expand scale and shift to match sequence dimension
  let scale := nn.unsqueeze scale 1  -- [batch, 1, hidden_size]
  let shift := nn.unsqueeze shift 1  -- [batch, 1, hidden_size]
  let scale := nn.expand scale #[batch, seq, hidden_size]
  let shift := nn.expand shift #[batch, seq, hidden_size]
  -- Apply: (1 + scale) * x + shift
  let scale := add_scalar scale 1.0
  scale * x + shift

end torch.flux
