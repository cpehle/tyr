/-
  Masked Diffusion Schedule for discrete diffusion training

  Implements a linear mask schedule where:
  - mask_prob = linspace(1/T, 1.0, T)
  - At timestep t: probability of masking = mask_probs[t]
  - Context tokens (first context_len) are never masked
-/
import Tyr.Torch

/-!
# `Examples.Diffusion.DiffusionSchedule`

Masked diffusion schedule logic that controls mask probability over training progress.

## Overview
- Example entrypoint intended for runnable end-to-end workflows.
- Uses markdown module docs so `doc-gen4` renders a readable module landing section.
-/

namespace torch.diffusion

open torch

/-- Masked diffusion schedule for training -/
structure MaskedDiffusionSchedule (num_timesteps : UInt64) where
  mask_token_id : UInt64
  context_len : UInt64
  /-- mask_probs[t] = probability of masking at timestep t -/
  mask_probs : T #[num_timesteps]

/-- Initialize a linear mask schedule -/
def MaskedDiffusionSchedule.init (num_timesteps mask_token_id context_len : UInt64)
    : MaskedDiffusionSchedule num_timesteps :=
  let start := 1.0 / num_timesteps.toFloat
  let stop := 1.0
  {
    mask_token_id := mask_token_id
    context_len := context_len
    mask_probs := linspace start stop num_timesteps
  }

/-- Add masks to clean tokens for training.
    Given clean tokens x_0 and timestep t (per sample),
    returns masked tokens x_t where random positions are replaced with mask_token_id.

    NOTE: Context tokens (first context_len positions) are never masked.
    The mask probability is timestep-dependent: higher timesteps = more masking.
-/
def MaskedDiffusionSchedule.addMasks {num_timesteps batch seq : UInt64}
    (schedule : MaskedDiffusionSchedule num_timesteps)
    (x_0 : T #[batch, seq])     -- Clean tokens
    (t : T #[batch])            -- Timestep per sample (int64)
    : IO (T #[batch, seq]) := do
  -- Get mask probability for each sample based on timestep
  -- mask_probs[t] gives the probability of masking at timestep t
  -- Higher t = more masking (linear schedule from 1/T to 1.0)
  let maskProbPerSample : T #[batch] := data.indexSelect schedule.mask_probs 0 t  -- [batch]

  -- Expand to [batch, seq] for comparison
  let maskProbExpanded := nn.unsqueeze maskProbPerSample 1  -- [batch, 1]
  let maskProbBroadcast := nn.expand maskProbExpanded #[batch, seq]  -- [batch, seq]

  -- Generate random values in [0, 1)
  let randTensor ‚Üê rand #[batch, seq]

  -- Create mask: true where we should mask (rand < mask_prob)
  let shouldMask := lt randTensor maskProbBroadcast  -- [batch, seq]

  -- Protect context tokens: create position mask
  let positionValues := linspace 0.0 (seq.toFloat - 1.0) seq
  let contextMask := lt_scalar positionValues schedule.context_len.toFloat  -- [seq], true for context

  -- Expand context mask to [batch, seq]
  let contextMask_unsq := nn.unsqueeze contextMask 0  -- [1, seq]
  let contextMask_exp := nn.expand contextMask_unsq #[batch, seq]  -- [batch, seq]

  -- Final mask: should mask AND not in context
  let notContext := logical_not contextMask_exp
  let finalMask := logical_and shouldMask notContext

  -- Apply mask: where finalMask is true, use mask_token_id, else original
  let maskTokens := full_int #[batch, seq] schedule.mask_token_id.toInt64
  let x_t := where_ finalMask maskTokens x_0

  return x_t

/-- Get a mask indicating which positions are masked in x_t -/
def getMaskedPositions {batch seq : UInt64}
    (x_t : T #[batch, seq])
    (mask_token_id : UInt64)
    : T #[batch, seq] :=
  eq_scalar x_t mask_token_id.toInt64

/-- Compute masked cross-entropy loss.
    Only computes loss on positions that were masked (x_t == mask_token_id).
    Returns the mean loss over masked positions.
-/
def maskedCrossEntropyLoss {batch seq vocab : UInt64}
    (logits : T #[batch, seq, vocab])   -- Model output logits
    (targets : T #[batch, seq])          -- Original clean tokens (x_0)
    (x_t : T #[batch, seq])              -- Masked input tokens
    (mask_token_id : UInt64)
    : T #[] :=
  -- Flatten for cross-entropy
  let logitsFlat := reshape logits #[batch * seq, vocab]
  let targetsFlat := reshape targets #[batch * seq]

  -- Compute per-token loss (reduction=none)
  let lossPerToken := nn.cross_entropy_none logitsFlat targetsFlat

  -- Get mask of which positions were masked
  let mask := eq_scalar x_t mask_token_id.toInt64  -- [batch, seq]
  let maskFlat := reshape mask #[batch * seq]
  let maskFloat := toFloat' maskFlat  -- Convert bool to float for multiplication

  -- Apply mask and compute mean over masked positions
  let maskedLoss := lossPerToken * maskFloat
  let totalLoss := nn.sumAll maskedLoss
  let numMasked := nn.sumAll maskFloat

  -- Return mean (total / count)
  -- Add small epsilon to avoid division by zero
  let numMaskedSafe := add_scalar numMasked 1e-8
  nn.div totalLoss numMaskedSafe

end torch.diffusion
